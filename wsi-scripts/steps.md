好的，我们来一步步计算出您的脚本训练一个 epoch 所需的总步数。

### 计算所需参数

首先，我们从您的 `pretrain.sh` 脚本和提供的信息中提取关键参数：

*   **总数据量**: 486,292 条
*   **GPU 数量** (`NPROC_PER_NODE`): 4
*   **单卡批次大小** (`per_device_train_batch_size`): 1
*   **梯度累积步数** (`gradient_accumulation_steps`): 1

### 计算步骤

#### 第一步：计算全局批次大小 (Global Batch Size)

全局批次大小是指在所有 GPU 上一次性（一个前向传播）处理的样本总数。

*   **公式**: `GPU 数量 * 单卡批次大小`
*   **计算**: `4 * 1 = 4`

这意味着，每次前向传播会同时处理 4 条数据。

#### 第二步：计算有效批次大小 (Effective Batch Size)

有效批次大小是指模型权重更新一次（即执行一次优化器步骤）所处理的样本总数。它考虑了梯度累积。

*   **公式**: `全局批次大小 * 梯度累积步数`
*   **计算**: `4 * 1 = 4`

这意味着，每处理 4 条数据，模型就会更新一次权重。

#### 第三步：计算总步数 (Total Steps per Epoch)

一个 epoch 的总训练步数，等于用总数据量除以有效批次大小。因为最后可能剩下不足一个批次的数据，所以结果需要向上取整。

*   **公式**: `向上取整 (总数据量 / 有效批次大小)`
*   **计算**: `ceil(486,292 / 4) = 121,573`

### 结论与建议

根据计算，您的训练任务跑完一个 epoch 需要 **121,573** 步。

既然我们已经精确计算出了步数，我建议您直接在 `pretrain.sh` 中使用这个值。这可以完美解决之前遇到的 `ValueError`，并且能让学习率调度器（Learning Rate Scheduler）按照一个完整 epoch 的长度精确地规划学习率衰减，这是最理想的情况。

您可以将脚本中的 `--max_epochs 1` 替换为：
`--max_steps 121573`